---
title: "Regression analysis - backwards selection"
author: "Petri Haavisto"
format: html
editor: visual
---

## Regression Analysis - Backwards selection

Libraries

```{r}
#| message: false
library(conflicted)  
library(tidyverse)
library(broom)
library(GGally)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
theme_set(theme_minimal()) #set preferred theme for plots
```

Data from Regression Analysis by Example by Chatterjee and Hadi (set P060).

```{r}
#| message: false
performance <- here::here("data/performance.csv") |> readr::read_csv()
View(performance)
```

In this data we are interested in overall rating (response variable) and how we can predict it from other variables like handles_employee_complaints. But first we want to check that assumptions are met that is that there is linear relationship between explaining variables and response variable (overall_rating). We can check this with ggpairs (GGally package):

```{r}
#| message: false
ggpairs(performance)
```

Here we look at graphs under overall_rating comared to other variables. There is nothing too alarming in this picture so we can continue...

We are going to build model like this:

-   First add all variables to the model

-   Remove them one by one

-   Remove the one that seems least significant

-   Keep going as long you have t-statistics under 1

We will use t-statistics to decide, which variable to throw away at each step. In the beginning we have 6 explanatory variables:

```{r}
model_6 <- lm(overall_rating ~ ., data = performance)
summary(model_6)
```

Here we look at the t-value. It tells us, how strong the evidence is against null hypothesis. The bigger the number is the more significant the variable is. So now we are going to remove the weakest one that is not_overly_critical. Notice the following handy update function for the updated model:

```{r}
model_5 <- update(model_6, .~. -not_overly_critical)
summary(model_5)
```

We remove the next weak one:

```{r}
model_4 <- update(model_5, .~. -raises_for_performance)
summary(model_4)
```

Only one value left with absolute value under 1:

```{r}
model_3 <- update(model_4, .~. -no_special_privileges)
summary(model_3)
```

Now all t-values are above 1 absolute value, so we have reached stopping criteria. So model_3 is our final model.

### Residual plot

Residual plot is excellent one stop diagnosis for your model. We use **augment** function from **Broom** package to help us. Here we get all the original variables plus:

-   fitted values (value of response value as provided by the model)

-   residual (how far away actual value is from prediction)

-   hat = diagonal of the hat matrix

-   sigma = estimated residual standard deviation when corresponding observation is dropped from model.

-   cooksd = Cooks distance

-   std.redid = standardized residuals

```{r}
perf_aug <- augment(model_3)
View(perf_aug)
```

Next we can use this data to plot residuals with ggplot.

```{r}
ggplot(perf_aug, aes(x = .fitted, y = .resid))+
  geom_point()+
  geom_hline(yintercept = 0) #to highlight x-axis
```

What to expect above?

1.  This should be just a cloud without any patterns.

2.  Spread should be even as we go left to right.

3.  There should not be any shapes (like parabolic shape or cubic shape).

So our residual plot looks pretty good!

### AIC - Akaike Information Criterion

AIC is used to compare different possible models and determine which one is the best fit for the data.

First we list all the models:

```{r}
model_list <- list(model_6,
                   model_5,
                   model_4,
                   model_3)
```

Then we apply function **extractAIC** to every model in the list:

```{r}
lapply(model_list, extractAIC)
```

Here smaller number is better. So we can see that on every step we took, our model got better and better and the final model (model_3 ) was the best.
